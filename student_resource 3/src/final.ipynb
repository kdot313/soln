{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b70b34e",
   "metadata": {},
   "source": [
    "### Basic library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719d15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8911e33",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3136aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "sample_test = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "sample_test_out = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test_out.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebd689",
   "metadata": {},
   "source": [
    "### Run Sanity check using src/sanity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bb3988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing successfull for file: ../dataset/sample_test_out.csv\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/sample_test.csv --output_filename ../dataset/sample_test_out.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa79459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Invalid unit [lbs] found in 6.75 lbs. Allowed units: {'yard', 'inch', 'metre', 'millivolt', 'microgram', 'ounce', 'kilovolt', 'milligram', 'kilogram', 'quart', 'decilitre', 'ton', 'kilowatt', 'watt', 'litre', 'pound', 'centilitre', 'microlitre', 'pint', 'cubic inch', 'volt', 'gram', 'foot', 'gallon', 'cubic foot', 'fluid ounce', 'imperial gallon', 'centimetre', 'cup', 'millimetre', 'millilitre'}\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/sample_test.csv --output_filename ../dataset/sample_test_out_fail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe930a8",
   "metadata": {},
   "source": [
    "### Splitting the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d1aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import download_images\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb57b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Solutions/student_resource 3\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "872b3e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/Solutions/student_resource 3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c38a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test.csv file\n",
    "df = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd0a7a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entity_name\n",
       "height                           32282\n",
       "depth                            28146\n",
       "width                            26931\n",
       "item_weight                      22032\n",
       "maximum_weight_recommendation     7028\n",
       "voltage                           5488\n",
       "wattage                           5447\n",
       "item_volume                       3833\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"entity_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3feb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows in each split\n",
    "split_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c4e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into 4 parts\n",
    "df_part1 = df.iloc[:split_size]\n",
    "df_part2 = df.iloc[split_size:2*split_size]\n",
    "df_part3 = df.iloc[2*split_size:3*split_size]\n",
    "df_part4 = df.iloc[3*split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71165bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV has been split into 4 parts successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save each part to a new CSV file\n",
    "df_part1.to_csv('test_part1.csv', index=False)\n",
    "df_part2.to_csv('test_part2.csv', index=False)\n",
    "df_part3.to_csv('test_part3.csv', index=False)\n",
    "df_part4.to_csv('test_part4.csv', index=False)\n",
    "\n",
    "print(\"CSV has been split into 4 parts successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e7643e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the stratified sample to a new CSV file\n",
    "stratified_sample.to_csv('stratified_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c93f8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder where images will be downloaded\n",
    "download_folder = 'downloaded_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7d815f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "from utils import download_images, parse_string, common_mistake\n",
    "import constants\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eead5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing function for images with contrast and optional skew correction\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # Apply CLAHE for contrast enhancement\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        enhanced_image = clahe.apply(gray)\n",
    "        # Apply thresholding\n",
    "        _, thresh = cv2.threshold(enhanced_image, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        # Apply median blur to remove noise\n",
    "        processed_image = cv2.medianBlur(thresh, 3)\n",
    "        return processed_image\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc55900",
   "metadata": {},
   "source": [
    "## Paddle-OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62947885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f326e58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /home/codespace/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.00M/4.00M [00:07<00:00, 510kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar to /home/codespace/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/en_PP-OCRv4_rec_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.2M/10.2M [00:03<00:00, 3.10MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 16:49:43] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/codespace/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/codespace/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/workspaces/Solutions/env/lib/python3.10/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/codespace/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the PaddleOCR model once\n",
    "ocr_paddleocr = PaddleOCR(use_angle_cls=True, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04263ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract text from image using PaddleOCR\n",
    "def extract_text_paddleocr(image_path):\n",
    "    try:\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        if preprocessed_image is None:\n",
    "            return \"\"\n",
    "        result = ocr_paddleocr.ocr(image_path)\n",
    "        extracted_text = ' '.join([line[1][0] for line in result[0]])\n",
    "        return extracted_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"PaddleOCR extraction failed for {image_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d74bb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Process images and save the extracted text\n",
    "def process_images_paddleocr(stratified_sample_file, download_folder, output_file):\n",
    "    df = pd.read_csv(stratified_sample_file)\n",
    "    print(\"Downloading images...\")\n",
    "    download_images(df['image_link'], download_folder)\n",
    "    \n",
    "    print(\"Extracting text using PaddleOCR...\")\n",
    "    df['Extracted'] = df['image_link'].apply(lambda link: extract_text_paddleocr(os.path.join(download_folder, os.path.basename(link))))\n",
    "    \n",
    "    # Clean up images after extraction\n",
    "    for file in os.listdir(download_folder):\n",
    "        os.remove(os.path.join(download_folder, file))\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcc9cea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 143.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text using PaddleOCR...\n",
      "[2024/09/15 16:50:10] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.3413219451904297\n",
      "[2024/09/15 16:50:10] ppocr DEBUG: cls num  : 2, elapsed : 0.02020430564880371\n",
      "[2024/09/15 16:50:10] ppocr DEBUG: rec_res num  : 2, elapsed : 0.09525585174560547\n",
      "[2024/09/15 16:50:11] ppocr DEBUG: dt_boxes num : 5, elapsed : 0.1657571792602539\n",
      "[2024/09/15 16:50:11] ppocr DEBUG: cls num  : 5, elapsed : 0.016232728958129883\n",
      "[2024/09/15 16:50:11] ppocr DEBUG: rec_res num  : 5, elapsed : 0.14435386657714844\n",
      "[2024/09/15 16:50:11] ppocr DEBUG: dt_boxes num : 7, elapsed : 0.17621970176696777\n",
      "[2024/09/15 16:50:11] ppocr DEBUG: cls num  : 7, elapsed : 0.029121875762939453\n",
      "[2024/09/15 16:50:12] ppocr DEBUG: rec_res num  : 7, elapsed : 0.564643144607544\n",
      "[2024/09/15 16:50:12] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.16322636604309082\n",
      "[2024/09/15 16:50:12] ppocr DEBUG: cls num  : 4, elapsed : 0.015394449234008789\n",
      "[2024/09/15 16:50:12] ppocr DEBUG: rec_res num  : 4, elapsed : 0.10903501510620117\n",
      "[2024/09/15 16:50:12] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.1865689754486084\n",
      "[2024/09/15 16:50:12] ppocr DEBUG: cls num  : 13, elapsed : 0.018203020095825195\n",
      "[2024/09/15 16:50:13] ppocr DEBUG: rec_res num  : 13, elapsed : 0.4978034496307373\n",
      "[2024/09/15 16:50:13] ppocr DEBUG: dt_boxes num : 24, elapsed : 0.181304931640625\n",
      "[2024/09/15 16:50:13] ppocr DEBUG: cls num  : 24, elapsed : 0.027857303619384766\n",
      "[2024/09/15 16:50:13] ppocr DEBUG: rec_res num  : 24, elapsed : 0.5354490280151367\n",
      "[2024/09/15 16:50:14] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.16810917854309082\n",
      "[2024/09/15 16:50:14] ppocr DEBUG: cls num  : 13, elapsed : 0.018349409103393555\n",
      "[2024/09/15 16:50:14] ppocr DEBUG: rec_res num  : 13, elapsed : 0.3593745231628418\n",
      "[2024/09/15 16:50:14] ppocr DEBUG: dt_boxes num : 7, elapsed : 0.1647958755493164\n",
      "[2024/09/15 16:50:14] ppocr DEBUG: cls num  : 7, elapsed : 0.022221088409423828\n",
      "[2024/09/15 16:50:14] ppocr DEBUG: rec_res num  : 7, elapsed : 0.17472267150878906\n",
      "[2024/09/15 16:50:15] ppocr DEBUG: dt_boxes num : 10, elapsed : 0.1615285873413086\n",
      "[2024/09/15 16:50:15] ppocr DEBUG: cls num  : 10, elapsed : 0.03498959541320801\n",
      "[2024/09/15 16:50:15] ppocr DEBUG: rec_res num  : 10, elapsed : 0.4461817741394043\n",
      "[2024/09/15 16:50:15] ppocr DEBUG: dt_boxes num : 5, elapsed : 0.17286372184753418\n",
      "[2024/09/15 16:50:15] ppocr DEBUG: cls num  : 5, elapsed : 0.009697675704956055\n",
      "[2024/09/15 16:50:15] ppocr DEBUG: rec_res num  : 5, elapsed : 0.17925691604614258\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: dt_boxes num : 10, elapsed : 0.09551644325256348\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: cls num  : 10, elapsed : 0.018072128295898438\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: rec_res num  : 10, elapsed : 0.3584003448486328\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.16225695610046387\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: cls num  : 2, elapsed : 0.012015342712402344\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: rec_res num  : 2, elapsed : 0.06924271583557129\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: dt_boxes num : 14, elapsed : 0.1626725196838379\n",
      "[2024/09/15 16:50:16] ppocr DEBUG: cls num  : 14, elapsed : 0.021063804626464844\n",
      "[2024/09/15 16:50:19] ppocr DEBUG: rec_res num  : 14, elapsed : 2.1508538722991943\n",
      "[2024/09/15 16:50:19] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.18516969680786133\n",
      "[2024/09/15 16:50:19] ppocr DEBUG: cls num  : 3, elapsed : 0.017923593521118164\n",
      "[2024/09/15 16:50:19] ppocr DEBUG: rec_res num  : 3, elapsed : 0.14299225807189941\n",
      "[2024/09/15 16:50:19] ppocr DEBUG: dt_boxes num : 8, elapsed : 0.21025967597961426\n",
      "[2024/09/15 16:50:19] ppocr DEBUG: cls num  : 8, elapsed : 0.018112897872924805\n",
      "[2024/09/15 16:50:19] ppocr DEBUG: rec_res num  : 8, elapsed : 0.3320009708404541\n",
      "[2024/09/15 16:50:20] ppocr DEBUG: dt_boxes num : 34, elapsed : 0.2386777400970459\n",
      "[2024/09/15 16:50:20] ppocr DEBUG: cls num  : 34, elapsed : 0.05721640586853027\n",
      "[2024/09/15 16:50:21] ppocr DEBUG: rec_res num  : 34, elapsed : 0.9731571674346924\n",
      "[2024/09/15 16:50:21] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.16149520874023438\n",
      "[2024/09/15 16:50:21] ppocr DEBUG: cls num  : 2, elapsed : 0.0055789947509765625\n",
      "[2024/09/15 16:50:21] ppocr DEBUG: rec_res num  : 2, elapsed : 0.07499027252197266\n",
      "[2024/09/15 16:50:21] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.16170692443847656\n",
      "[2024/09/15 16:50:21] ppocr DEBUG: cls num  : 3, elapsed : 0.00807046890258789\n",
      "[2024/09/15 16:50:21] ppocr DEBUG: rec_res num  : 3, elapsed : 0.10010743141174316\n",
      "[2024/09/15 16:50:22] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.2149372100830078\n",
      "[2024/09/15 16:50:22] ppocr DEBUG: cls num  : 2, elapsed : 0.005326986312866211\n",
      "[2024/09/15 16:50:22] ppocr DEBUG: rec_res num  : 2, elapsed : 0.05643916130065918\n",
      "[2024/09/15 16:50:22] ppocr DEBUG: dt_boxes num : 28, elapsed : 0.13930606842041016\n",
      "[2024/09/15 16:50:22] ppocr DEBUG: cls num  : 28, elapsed : 0.03853464126586914\n",
      "[2024/09/15 16:50:25] ppocr DEBUG: rec_res num  : 28, elapsed : 3.022848129272461\n",
      "[2024/09/15 16:50:25] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.2393336296081543\n",
      "[2024/09/15 16:50:25] ppocr DEBUG: cls num  : 6, elapsed : 0.010356664657592773\n",
      "[2024/09/15 16:50:26] ppocr DEBUG: rec_res num  : 6, elapsed : 0.27307796478271484\n",
      "[2024/09/15 16:50:26] ppocr DEBUG: dt_boxes num : 15, elapsed : 0.17089295387268066\n",
      "[2024/09/15 16:50:26] ppocr DEBUG: cls num  : 15, elapsed : 0.03482842445373535\n",
      "[2024/09/15 16:50:26] ppocr DEBUG: rec_res num  : 15, elapsed : 0.5262341499328613\n",
      "[2024/09/15 16:50:27] ppocr DEBUG: dt_boxes num : 53, elapsed : 0.20072269439697266\n",
      "[2024/09/15 16:50:27] ppocr DEBUG: cls num  : 53, elapsed : 0.07681155204772949\n",
      "[2024/09/15 16:50:30] ppocr DEBUG: rec_res num  : 53, elapsed : 3.0880043506622314\n",
      "[2024/09/15 16:50:30] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.1221013069152832\n",
      "[2024/09/15 16:50:30] ppocr DEBUG: cls num  : 9, elapsed : 0.028290271759033203\n",
      "[2024/09/15 16:50:30] ppocr DEBUG: rec_res num  : 9, elapsed : 0.24821090698242188\n",
      "[2024/09/15 16:50:30] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.20422816276550293\n",
      "[2024/09/15 16:50:30] ppocr DEBUG: cls num  : 3, elapsed : 0.00879669189453125\n",
      "[2024/09/15 16:50:31] ppocr DEBUG: rec_res num  : 3, elapsed : 0.07977724075317383\n",
      "[2024/09/15 16:50:31] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.2169053554534912\n",
      "[2024/09/15 16:50:31] ppocr DEBUG: cls num  : 12, elapsed : 0.01734447479248047\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: rec_res num  : 12, elapsed : 0.6900496482849121\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.16089963912963867\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: cls num  : 3, elapsed : 0.008756160736083984\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: rec_res num  : 3, elapsed : 0.07993888854980469\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.16099953651428223\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: cls num  : 6, elapsed : 0.010196685791015625\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: rec_res num  : 6, elapsed : 0.14882135391235352\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: dt_boxes num : 17, elapsed : 0.21639299392700195\n",
      "[2024/09/15 16:50:32] ppocr DEBUG: cls num  : 17, elapsed : 0.036043643951416016\n",
      "[2024/09/15 16:50:33] ppocr DEBUG: rec_res num  : 17, elapsed : 0.4354057312011719\n",
      "[2024/09/15 16:50:33] ppocr DEBUG: dt_boxes num : 24, elapsed : 0.1720104217529297\n",
      "[2024/09/15 16:50:33] ppocr DEBUG: cls num  : 24, elapsed : 0.030582427978515625\n",
      "[2024/09/15 16:50:34] ppocr DEBUG: rec_res num  : 24, elapsed : 0.5819416046142578\n",
      "[2024/09/15 16:50:34] ppocr DEBUG: dt_boxes num : 27, elapsed : 0.17154979705810547\n",
      "[2024/09/15 16:50:34] ppocr DEBUG: cls num  : 27, elapsed : 0.0374760627746582\n",
      "[2024/09/15 16:50:35] ppocr DEBUG: rec_res num  : 27, elapsed : 0.7731008529663086\n",
      "[2024/09/15 16:50:35] ppocr DEBUG: dt_boxes num : 24, elapsed : 0.16729450225830078\n",
      "[2024/09/15 16:50:35] ppocr DEBUG: cls num  : 24, elapsed : 0.0312502384185791\n",
      "[2024/09/15 16:50:36] ppocr DEBUG: rec_res num  : 24, elapsed : 0.9640312194824219\n",
      "[2024/09/15 16:50:36] ppocr DEBUG: dt_boxes num : 7, elapsed : 0.16613984107971191\n",
      "[2024/09/15 16:50:36] ppocr DEBUG: cls num  : 7, elapsed : 0.02710270881652832\n",
      "[2024/09/15 16:50:36] ppocr DEBUG: rec_res num  : 7, elapsed : 0.3681912422180176\n",
      "[2024/09/15 16:50:37] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.16501402854919434\n",
      "[2024/09/15 16:50:37] ppocr DEBUG: cls num  : 13, elapsed : 0.024947166442871094\n",
      "[2024/09/15 16:50:38] ppocr DEBUG: rec_res num  : 13, elapsed : 0.8357386589050293\n",
      "[2024/09/15 16:50:38] ppocr DEBUG: dt_boxes num : 14, elapsed : 0.19493818283081055\n",
      "[2024/09/15 16:50:38] ppocr DEBUG: cls num  : 14, elapsed : 0.025364398956298828\n",
      "[2024/09/15 16:50:38] ppocr DEBUG: rec_res num  : 14, elapsed : 0.4676840305328369\n",
      "[2024/09/15 16:50:39] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.18010640144348145\n",
      "[2024/09/15 16:50:39] ppocr DEBUG: cls num  : 4, elapsed : 0.019555091857910156\n",
      "[2024/09/15 16:50:39] ppocr DEBUG: rec_res num  : 4, elapsed : 0.12183284759521484\n",
      "[2024/09/15 16:50:39] ppocr DEBUG: dt_boxes num : 15, elapsed : 0.21365904808044434\n",
      "[2024/09/15 16:50:39] ppocr DEBUG: cls num  : 15, elapsed : 0.03531622886657715\n",
      "[2024/09/15 16:50:40] ppocr DEBUG: rec_res num  : 15, elapsed : 0.6061151027679443\n",
      "[2024/09/15 16:50:40] ppocr DEBUG: dt_boxes num : 11, elapsed : 0.16339349746704102\n",
      "[2024/09/15 16:50:40] ppocr DEBUG: cls num  : 11, elapsed : 0.028829097747802734\n",
      "[2024/09/15 16:50:40] ppocr DEBUG: rec_res num  : 11, elapsed : 0.2975490093231201\n",
      "[2024/09/15 16:50:40] ppocr DEBUG: dt_boxes num : 29, elapsed : 0.1719226837158203\n",
      "[2024/09/15 16:50:40] ppocr DEBUG: cls num  : 29, elapsed : 0.03898787498474121\n",
      "[2024/09/15 16:50:41] ppocr DEBUG: rec_res num  : 29, elapsed : 0.8841135501861572\n",
      "[2024/09/15 16:50:41] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.16062140464782715\n",
      "[2024/09/15 16:50:41] ppocr DEBUG: cls num  : 2, elapsed : 0.008102655410766602\n",
      "[2024/09/15 16:50:42] ppocr DEBUG: rec_res num  : 2, elapsed : 0.07748961448669434\n",
      "[2024/09/15 16:50:42] ppocr DEBUG: dt_boxes num : 22, elapsed : 0.1813511848449707\n",
      "[2024/09/15 16:50:42] ppocr DEBUG: cls num  : 22, elapsed : 0.0416867733001709\n",
      "[2024/09/15 16:50:42] ppocr DEBUG: rec_res num  : 22, elapsed : 0.6975183486938477\n",
      "[2024/09/15 16:50:43] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.17960500717163086\n",
      "[2024/09/15 16:50:43] ppocr DEBUG: cls num  : 13, elapsed : 0.03314018249511719\n",
      "[2024/09/15 16:50:43] ppocr DEBUG: rec_res num  : 13, elapsed : 0.5155532360076904\n",
      "[2024/09/15 16:50:43] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.1775805950164795\n",
      "[2024/09/15 16:50:43] ppocr DEBUG: cls num  : 13, elapsed : 0.023723840713500977\n",
      "[2024/09/15 16:50:44] ppocr DEBUG: rec_res num  : 13, elapsed : 0.7275040149688721\n",
      "[2024/09/15 16:50:44] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.16156911849975586\n",
      "[2024/09/15 16:50:44] ppocr DEBUG: cls num  : 6, elapsed : 0.010131359100341797\n",
      "[2024/09/15 16:50:45] ppocr DEBUG: rec_res num  : 6, elapsed : 0.3183913230895996\n",
      "[2024/09/15 16:50:45] ppocr DEBUG: dt_boxes num : 8, elapsed : 0.18260669708251953\n",
      "[2024/09/15 16:50:45] ppocr DEBUG: cls num  : 8, elapsed : 0.0175323486328125\n",
      "[2024/09/15 16:50:45] ppocr DEBUG: rec_res num  : 8, elapsed : 0.2458031177520752\n",
      "[2024/09/15 16:50:45] ppocr DEBUG: dt_boxes num : 18, elapsed : 0.17233014106750488\n",
      "[2024/09/15 16:50:45] ppocr DEBUG: cls num  : 18, elapsed : 0.02440786361694336\n",
      "[2024/09/15 16:50:46] ppocr DEBUG: rec_res num  : 18, elapsed : 0.554173469543457\n",
      "[2024/09/15 16:50:46] ppocr DEBUG: dt_boxes num : 14, elapsed : 0.18102741241455078\n",
      "[2024/09/15 16:50:46] ppocr DEBUG: cls num  : 14, elapsed : 0.022082090377807617\n",
      "[2024/09/15 16:50:47] ppocr DEBUG: rec_res num  : 14, elapsed : 0.4079732894897461\n",
      "[2024/09/15 16:50:47] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.16072797775268555\n",
      "[2024/09/15 16:50:47] ppocr DEBUG: cls num  : 4, elapsed : 0.019811391830444336\n",
      "[2024/09/15 16:50:47] ppocr DEBUG: rec_res num  : 4, elapsed : 0.38959360122680664\n",
      "[2024/09/15 16:50:47] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.17899274826049805\n",
      "[2024/09/15 16:50:47] ppocr DEBUG: cls num  : 12, elapsed : 0.017433881759643555\n",
      "[2024/09/15 16:50:50] ppocr DEBUG: rec_res num  : 12, elapsed : 2.3988800048828125\n",
      "[2024/09/15 16:50:50] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.17125940322875977\n",
      "[2024/09/15 16:50:50] ppocr DEBUG: cls num  : 3, elapsed : 0.023791074752807617\n",
      "[2024/09/15 16:50:50] ppocr DEBUG: rec_res num  : 3, elapsed : 0.10478401184082031\n",
      "Results saved to output_paddleocr.csv\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "process_images_paddleocr(\"stratified_sample.csv\", \"downloaded_images\", \"output_paddleocr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9a711",
   "metadata": {},
   "source": [
    "## Mistral-Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16621ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4359b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain_mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6e48ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"]=os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b3b6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5878069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb9b91d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Narendra Modi is an Indian politician who has been serving as the Prime Minister of India since 2014. Here are some key points about him:\\n\\n1. **Full Name**: Narendra Damodardas Modi\\n2. **Birthdate**: September 17, 1950\\n3. **Birthplace**: Vadnagar, Gujarat, India\\n4. **Political Party**: Bharatiya Janata Party (BJP)\\n5. **Previous Offices**:\\n   - Chief Minister of Gujarat (2001-2014)\\n   - Member of Parliament from Varanasi (2014-present)\\n6. **Education**: Modi holds a Bachelor's degree in Political Science from School of Open Learning, University of Delhi, and a Master's degree in Political Science from Gujarat University.\\n7. **Policies and Initiatives**: As Prime Minister, Modi has launched several initiatives, including:\\n   - Swachh Bharat Abhiyan (Clean India Mission)\\n   - Make in India\\n   - Digital India\\n   - Demonetization (currency note ban) in 2016\\n   - Goods and Services Tax (GST) implementation\\n8. **International Recognition**: Modi has been recognized internationally, including being awarded the 'Champion of the Earth' award by the United Nations in 2018 and the 'Global Goalkeeper' award by the Bill and Melinda Gates Foundation in 2019.\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 11, 'total_tokens': 360, 'completion_tokens': 349}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-b688005b-10f1-4d11-814d-1eb6a8f2e5fe-0', usage_metadata={'input_tokens': 11, 'output_tokens': 349, 'total_tokens': 360})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"who is narender modi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5805b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry decorator function with increased wait time\n",
    "def retry_on_rate_limit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        retries = 5  # Number of retries\n",
    "        wait_time = 120  # Start with 2 minutes wait time\n",
    "        \n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code == 429:\n",
    "                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    wait_time *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    raise e\n",
    "        raise Exception(\"Max retries reached. Failed due to rate limiting.\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77afa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3e97a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69d31ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the value and unit as required\n",
    "def format_value_unit(value, unit):\n",
    "    # Ensure whole numbers are formatted with .0\n",
    "    if float(value).is_integer():\n",
    "        value = f\"{float(value):.1f}\"\n",
    "\n",
    "    # Normalize the unit based on entity_unit_map\n",
    "    unit_mapping = {\n",
    "        \"mm\": \"millimetre\",\n",
    "        \"cm\": \"centimetre\",\n",
    "        \"m\": \"metre\",\n",
    "        \"in\": \"inch\",\n",
    "        \"g\": \"gram\",\n",
    "        \"kg\": \"kilogram\",\n",
    "        \"mg\": \"milligram\",\n",
    "        \"oz\": \"ounce\",\n",
    "        \"lb\": \"pound\",\n",
    "        \"ton\": \"ton\",\n",
    "        \"kv\": \"kilovolt\",\n",
    "        \"mv\": \"millivolt\",\n",
    "        \"v\": \"volt\",\n",
    "        \"kw\": \"kilowatt\",\n",
    "        \"w\": \"watt\",\n",
    "        # Add more as needed based on your entity_unit_map\n",
    "    }\n",
    "\n",
    "    # Convert the unit to its full form\n",
    "    unit = unit_mapping.get(unit.lower(), unit)\n",
    "\n",
    "    return f\"{value} {unit}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b35f6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the LLM response and apply post-processing\n",
    "def process_llm_response(response):\n",
    "    response_text = response.content.strip()\n",
    "\n",
    "    # If the response includes irrelevant text (e.g., \"no mention of value\"), return empty string\n",
    "    if \"no mention\" in response_text.lower() or not response_text:\n",
    "        return \"\"\n",
    "\n",
    "    # Extract value and unit from the response using regex\n",
    "    import re\n",
    "    match = re.search(r'(\\d+\\.?\\d*)\\s*([a-zA-Z]+)', response_text)\n",
    "    if match:\n",
    "        value, unit = match.groups()\n",
    "        # Apply formatting\n",
    "        return format_value_unit(value, unit)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ce9d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the prompt using LangChain's PromptTemplate\n",
    "def create_prompt(entity_name, extracted_text, allowed_units):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"entity_name\", \"allowed_units\", \"extracted_text\"],\n",
    "        template=\"\"\"\n",
    "        Extract the value and unit for the given entity based on the following text.\n",
    "        Return only the value and unit in the format: \"<value> <unit>\".\n",
    "\n",
    "        ### Entity Name:\n",
    "        {entity_name}\n",
    "\n",
    "        ### Allowed Units:\n",
    "        {allowed_units}\n",
    "\n",
    "        ### Text:\n",
    "        \"{extracted_text}\"\n",
    "\n",
    "        ### Expected Output:\n",
    "        \"\"\"\n",
    "    )\n",
    "    return prompt_template.format(\n",
    "        entity_name=entity_name,\n",
    "        allowed_units=', '.join(allowed_units),\n",
    "        extracted_text=extracted_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8ded385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the predict column with retry logic\n",
    "@retry_on_rate_limit\n",
    "def get_value_from_llm(entity_name, extracted_text):\n",
    "    allowed_units = entity_unit_map.get(entity_name, [])\n",
    "    if not allowed_units:\n",
    "        return \"\"\n",
    "\n",
    "    # Generate the prompt using LangChain\n",
    "    prompt = create_prompt(entity_name, extracted_text, allowed_units)\n",
    "    \n",
    "    # Send the prompt to the LLM using .invoke() and get the response\n",
    "    response = llm.invoke(prompt)  # Correctly call the Mistral LLM\n",
    "    \n",
    "    # Post-process the LLM's response\n",
    "    entity_value = process_llm_response(response)\n",
    "    \n",
    "    return entity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08e0c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process CSV in batches with delay between batches\n",
    "def process_csv_in_batches(input_csv, output_csv, batch_size=20, wait_time=300):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Initialize a new column for predictions\n",
    "    df['predict'] = \"\"\n",
    "\n",
    "    # Process in batches\n",
    "    num_batches = (len(df) // batch_size) + 1\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        print(f\"Processing batch {batch_num + 1}/{num_batches}\")\n",
    "\n",
    "        for idx, row in batch_df.iterrows():\n",
    "            entity_name = row['entity_name']\n",
    "            extracted_text = row['Extracted']\n",
    "            \n",
    "            # Get the value and unit using the Mistral LLM\n",
    "            entity_value = get_value_from_llm(entity_name, extracted_text)\n",
    "\n",
    "            # Insert the generated entity_value into the 'predict' column\n",
    "            df.at[idx, 'predict'] = entity_value\n",
    "\n",
    "        # Save after each batch to avoid losing data in case of interruption\n",
    "        df.to_csv(output_csv, index=False)\n",
    "\n",
    "        # Wait between batches to avoid hitting the rate limit\n",
    "        print(f\"Batch {batch_num + 1} processed. Waiting {wait_time} seconds before processing the next batch.\")\n",
    "        time.sleep(wait_time)  # Wait before processing the next batch\n",
    "\n",
    "    print(f\"Finished processing {num_batches} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1e291ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"output_paddleocr.csv\"\n",
    "output_csv = \"output_with_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b5bdc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/21\n",
      "Batch 1 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 2/21\n",
      "Batch 2 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 3/21\n",
      "Batch 3 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 4/21\n",
      "Batch 4 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 5/21\n",
      "Batch 5 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 6/21\n",
      "Batch 6 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 7/21\n",
      "Batch 7 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 8/21\n",
      "Batch 8 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 9/21\n",
      "Batch 9 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 10/21\n",
      "Batch 10 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 11/21\n",
      "Batch 11 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 12/21\n",
      "Batch 12 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 13/21\n",
      "Batch 13 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 14/21\n",
      "Batch 14 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 15/21\n",
      "Batch 15 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 16/21\n",
      "Batch 16 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 17/21\n",
      "Batch 17 processed. Waiting 40 seconds before processing the next batch.\n",
      "Processing batch 18/21\n"
     ]
    },
    {
     "ename": "HTTPStatusError",
     "evalue": "Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_csv_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 23\u001b[0m, in \u001b[0;36mprocess_csv_in_batches\u001b[0;34m(input_csv, output_csv, batch_size, wait_time)\u001b[0m\n\u001b[1;32m     20\u001b[0m extracted_text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the value and unit using the Mistral LLM\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m entity_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_value_from_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextracted_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Insert the generated entity_value into the 'predict' column\u001b[39;00m\n\u001b[1;32m     26\u001b[0m df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m entity_value\n",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m, in \u001b[0;36mretry_on_rate_limit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n",
      "Cell \u001b[0;32mIn[60], line 12\u001b[0m, in \u001b[0;36mget_value_from_llm\u001b[0;34m(entity_name, extracted_text)\u001b[0m\n\u001b[1;32m      9\u001b[0m prompt \u001b[38;5;241m=\u001b[39m create_prompt(entity_name, extracted_text, allowed_units)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Send the prompt to the LLM using .invoke() and get the response\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Correctly call the Mistral LLM\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Post-process the LLM's response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m entity_value \u001b[38;5;241m=\u001b[39m process_llm_response(response)\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:855\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_mistralai/chat_models.py:537\u001b[0m, in \u001b[0;36mChatMistralAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    536\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 537\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_mistralai/chat_models.py:456\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m         _raise_on_error(response)\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m--> 456\u001b[0m rtn \u001b[38;5;241m=\u001b[39m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rtn\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_mistralai/chat_models.py:453\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 453\u001b[0m     \u001b[43m_raise_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/workspaces/Solutions/env/lib/python3.10/site-packages/langchain_mistralai/chat_models.py:170\u001b[0m, in \u001b[0;36m_raise_on_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mis_error(response\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[1;32m    169\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError(\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError response \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    173\u001b[0m         request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m    174\u001b[0m         response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    175\u001b[0m     )\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}"
     ]
    }
   ],
   "source": [
    "process_csv_in_batches(input_csv, output_csv, batch_size=50, wait_time=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b258f",
   "metadata": {},
   "source": [
    "## LLM(LLAMA3.1)-Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d21b96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8592bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%##O#- #                                                                                                                                   1.8%#     97.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e7bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "ollama_thread = threading.Thread(target=ollama)\n",
    "ollama_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b14d7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/15 09:39:49 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/codespace/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
      "time=2024-09-15T09:39:49.662Z level=INFO source=images.go:753 msg=\"total blobs: 5\"\n",
      "time=2024-09-15T09:39:49.662Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-09-15T09:39:49.662Z level=INFO source=routes.go:1172 msg=\"Listening on [::]:11434 (version 0.3.10)\"\n",
      "time=2024-09-15T09:39:49.663Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama3191013085/runners\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/09/15 - 09:40:04 | 200 |      32.109µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2024/09/15 - 09:40:04 | 200 |   20.205479ms |       127.0.0.1 | POST     \"/api/show\"\n",
      "INFO [main] build info | build=1 commit=\"8962422\" tid=\"132312726518720\" timestamp=1726393204\n",
      "INFO [main] system info | n_threads=2 n_threads_batch=2 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"132312726518720\" timestamp=1726393204 total_threads=4\n",
      "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"42013\" tid=\"132312726518720\" timestamp=1726393204\n",
      "\u001b[?25l⠙ \u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-09-15T09:40:04.723Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm_v60102 cpu]\"\n",
      "time=2024-09-15T09:40:04.723Z level=INFO source=gpu.go:200 msg=\"looking for compatible GPUs\"\n",
      "time=2024-09-15T09:40:04.740Z level=INFO source=gpu.go:347 msg=\"no compatible GPUs were discovered\"\n",
      "time=2024-09-15T09:40:04.740Z level=INFO source=types.go:107 msg=\"inference compute\" id=0 library=cpu variant=avx2 compute=\"\" driver=0.0 name=\"\" total=\"15.6 GiB\" available=\"7.7 GiB\"\n",
      "time=2024-09-15T09:40:04.817Z level=INFO source=server.go:101 msg=\"system memory\" total=\"15.6 GiB\" free=\"7.7 GiB\" free_swap=\"0 B\"\n",
      "time=2024-09-15T09:40:04.818Z level=INFO source=memory.go:326 msg=\"offload to cpu\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[7.7 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[5.8 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
      "time=2024-09-15T09:40:04.819Z level=INFO source=server.go:391 msg=\"starting llama server\" cmd=\"/tmp/ollama3191013085/runners/cpu_avx2/ollama_llama_server --model /home/codespace/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --parallel 4 --port 42013\"\n",
      "time=2024-09-15T09:40:04.820Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\n",
      "time=2024-09-15T09:40:04.820Z level=INFO source=server.go:590 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-09-15T09:40:04.820Z level=INFO source=server.go:624 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/codespace/.ollama/models/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "time=2024-09-15T09:40:05.071Z level=INFO source=server.go:624 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4437.81 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     2.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25hINFO [main] model loaded | tid=\"132312726518720\" timestamp=1726393217\n",
      "\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h[GIN] 2024/09/15 - 09:40:17 | 200 | 12.864573209s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-09-15T09:40:17.626Z level=INFO source=server.go:629 msg=\"llama runner started in 12.81 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\n",
      "Use Ctrl + d or /bye to exit.\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ollama run llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea241e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e78fdbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/09/15 - 09:41:01 | 200 | 26.971848962s |             ::1 | POST     \"/api/generate\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The President of India as of my last update in 2021 was Droupadi Murmu. She is the first person from the indigenous people of Jharkhand (the Santal Pargana) and also the second woman to hold this office. Her tenure began on July 25, 2022. Prior to her presidency, she served as the Governor of Jharkhand from May 2015 until July 2021.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"who is president of india?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58222bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the prompt for LLaMA 3.1\n",
    "def generate_prompt(entity_name, extracted_text, allowed_units):\n",
    "    return f\"\"\"\n",
    "    Extract the value and unit for the given entity based on the following text.\n",
    "    Return only the value and unit in the format: \"<value> <unit>\".\n",
    "\n",
    "    ### Entity Name:\n",
    "    {entity_name}\n",
    "\n",
    "    ### Allowed Units:\n",
    "    {', '.join(allowed_units)}\n",
    "\n",
    "    ### Text:\n",
    "    \"{extracted_text}\"\n",
    "\n",
    "    ### Expected Output:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aa69f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query LLaMA 3.1 via Ollama\n",
    "def query_llama_ollama(prompt):\n",
    "    response = llm(prompt)  # Using langchain Ollama integration\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fac465f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process CSV with LLaMA 3.1 via Ollama\n",
    "def process_csv_with_llama(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Initialize a new column for predictions\n",
    "    df['predict'] = \"\"\n",
    "\n",
    "    # Loop over each row and generate the predict value\n",
    "    for idx, row in df.iterrows():\n",
    "        entity_name = row['entity_name']\n",
    "        extracted_text = row['Extracted']\n",
    "        \n",
    "        allowed_units = entity_unit_map.get(entity_name, [])\n",
    "        if not allowed_units:\n",
    "            df.at[idx, 'predict'] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Generate the prompt for each row\n",
    "        prompt = generate_prompt(entity_name, extracted_text, allowed_units)\n",
    "        \n",
    "        # Get the prediction from LLaMA 3.1 via Ollama\n",
    "        response = query_llama_ollama(prompt)\n",
    "        \n",
    "        # Insert the response into the predict column\n",
    "        df.at[idx, 'predict'] = response\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV\n",
    "    df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61c2285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example entity_unit_map\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb75a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_csv = \"output_paddleocr.csv\"\n",
    "output_csv = \"output_with_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6382fa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16049/2729794801.py:3: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm(prompt)  # Using langchain Ollama integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/09/15 - 09:41:21 | 200 | 20.361383189s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:41:40 | 200 | 18.621085989s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:42:17 | 200 | 37.669356193s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:43:11 | 200 | 53.277146063s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:43:26 | 200 | 15.825207096s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:43:47 | 200 | 20.349248119s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:43:58 | 200 | 11.145292842s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:44:37 | 200 |  38.98120871s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:45:07 | 200 | 30.221182432s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:45:19 | 200 | 11.565085136s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:45:41 | 200 | 21.781266882s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:45:53 | 200 | 12.801522176s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:46:19 | 200 | 25.281450257s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:47:51 | 200 |         1m32s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:48:27 | 200 | 35.677230175s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:48:55 | 200 | 27.837572501s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:49:21 | 200 | 26.273250066s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:49:59 | 200 | 37.757185164s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:50:11 | 200 | 12.205473778s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:51:14 | 200 |          1m3s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:51:26 | 200 | 11.497121335s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:51:57 | 200 |   30.6883497s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:53:21 | 200 |         1m24s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:54:01 | 200 | 39.445067087s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:54:52 | 200 | 51.157212619s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:55:11 | 200 | 19.101469082s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:55:43 | 200 | 32.105600213s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:55:51 | 200 |  8.109501102s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:56:03 | 200 |  11.34964823s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:56:38 | 200 | 35.645562303s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:57:09 | 200 | 31.189652037s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:57:32 | 200 | 22.677509695s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:57:44 | 200 | 11.777671618s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:57:59 | 200 | 14.825422891s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:58:11 | 200 | 12.269410086s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:58:32 | 200 | 20.765450883s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:58:49 | 200 | 17.449540781s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:59:17 | 200 |  27.55733861s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:59:39 | 200 | 22.569520969s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 09:59:51 | 200 | 11.597543523s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:00:39 | 200 | 48.209590324s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:00:54 | 200 | 14.973570173s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:01:11 | 200 | 17.305528619s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:01:29 | 200 | 17.861562463s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:01:55 | 200 |  25.89363197s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:02:12 | 200 |  16.53746604s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:02:40 | 200 | 27.857308046s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:02:53 | 200 | 13.857604734s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:03:40 | 200 | 46.365714223s |             ::1 | POST     \"/api/generate\"\n",
      "[GIN] 2024/09/15 - 10:03:51 | 200 | 11.184687066s |             ::1 | POST     \"/api/generate\"\n"
     ]
    }
   ],
   "source": [
    "# Process the CSV and generate the predict column using LLaMA 3.1 through Ollama\n",
    "process_csv_with_llama(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd336e0",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be579083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(df, entity_value_col='entity_value', predict_col='predict'):\n",
    "    # Initialize counts\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        GT = row[entity_value_col]\n",
    "        OUT = row[predict_col]\n",
    "\n",
    "        # Apply classification logic based on GT and OUT\n",
    "        if OUT != \"\" and GT != \"\":\n",
    "            if OUT == GT:\n",
    "                true_positives += 1  # True Positive\n",
    "            else:\n",
    "                false_positives += 1  # False Positive (OUT != GT)\n",
    "        elif OUT != \"\" and GT == \"\":\n",
    "            false_positives += 1  # False Positive (OUT != \"\" but GT is \"\")\n",
    "        elif OUT == \"\" and GT != \"\":\n",
    "            false_negatives += 1  # False Negative (OUT is \"\" but GT != \"\")\n",
    "        elif OUT == \"\" and GT == \"\":\n",
    "            true_negatives += 1  # True Negative\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    # Print results\n",
    "    print(f\"True Positives: {true_positives}\")\n",
    "    print(f\"False Positives: {false_positives}\")\n",
    "    print(f\"False Negatives: {false_negatives}\")\n",
    "    print(f\"True Negatives: {true_negatives}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84079280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 0\n",
      "False Positives: 50\n",
      "False Negatives: 0\n",
      "True Negatives: 0\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "df = pd.read_csv(\"output_with_predictions.csv\")  # Load your DataFrame\n",
    "f1_score = calculate_f1_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76032c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
